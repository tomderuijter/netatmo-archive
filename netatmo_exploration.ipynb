{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# System modules\n",
    "import pickle\n",
    "import folium\n",
    "from folium import plugins\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from scipy.stats import norm\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# User modules\n",
    "from domain.base import DataRequest\n",
    "from domain.base import Station\n",
    "import domain.file_io as file_io\n",
    "import domain.mongodb_engine as mongodb\n",
    "from domain.elevation_service import (\n",
    "    ElevationServiceConnector\n",
    ")\n",
    "\n",
    "from helpers.utils import (\n",
    "    get_station_coordinates,\n",
    "    add_station_elevations,\n",
    "    select_near,\n",
    "    add_alias\n",
    ")\n",
    "import helpers.knmi_obs_ingest as knmi\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_markers_on_map(station_dict):\n",
    "    map_1 = folium.Map(\n",
    "        [51.3, 5.6],\n",
    "        zoom_start=10, \n",
    "#         tiles='stamentoner'\n",
    "    )\n",
    "\n",
    "    for station_id in station_dict:\n",
    "        station = station_dict[station_id]\n",
    "        folium.Marker(\n",
    "            [station.latitude, station.longitude], \n",
    "            popup=\"Station id: %s\" % station.station_id,\n",
    "            icon=folium.Icon(color=station.color)\n",
    "        ).add_to(map_1)\n",
    "    return map_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read station meta data\n",
    "with open('data/stations.csv', 'r') as f:\n",
    "    station_meta_data = pd.read_csv(f)\n",
    "print(\"Loaded meta data for %d stations.\" % len(station_meta_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining a request for the Netherlands\n",
    "request = DataRequest()\n",
    "start_dt = datetime(2016, 4, 1, 0, 0)\n",
    "end_dt = datetime(2016, 5, 1, 0, 0)\n",
    "request.start_datetime = start_dt\n",
    "request.end_datetime = end_dt\n",
    "request.time_resolution = 10\n",
    "request.region = (52.000, 4.790, 51.880, 5.080)  # Cabauw area\n",
    "# request.region = (53.680, 2.865, 50.740, 7.323)  # The Netherlands\n",
    "print(\"Request defined from %s to %s\" % (start_dt, end_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining a request for the Netherlands\n",
    "from domain.base import DataRequest\n",
    "request = DataRequest()\n",
    "start_dt = datetime(2016, 4, 1, 0, 0)\n",
    "end_dt = datetime(2016, 5, 1, 0, 0)\n",
    "request.start_datetime = start_dt\n",
    "request.end_datetime = end_dt\n",
    "request.time_resolution = 10\n",
    "request.region = (53.680, 2.865, 50.740, 7.323)\n",
    "print(\"Request defined from %s to %s\" % (start_dt, end_dt))\n",
    "\n",
    "knmi_path = '/Volumes/various/netatmo/obs/'\n",
    "knmi_fsengine = knmi.FileSystemEngine(knmi_path)\n",
    "obs_data = knmi_fsengine.query(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load KNMI 10-minute observations from disk.\n",
    "print(\"Querying KNMI observation data..\")\n",
    "knmi_path = '/Volumes/various/netatmo/obs/'\n",
    "knmi_fsengine = knmi.FileSystemEngine(knmi_path)\n",
    "obs_data = knmi_fsengine.query(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump observation data to a single file for caching.\n",
    "with open('data/netherlands_obs_3-25_4-15.pkl', 'wb') as f:\n",
    "    pickle.dump(obs_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Load cached observations.\n",
    "# load_start = time()\n",
    "# with open('data/netherlands_obs_3-25_4-15.pkl', 'rb') as f:\n",
    "#     obs_data = pickle.load(f)\n",
    "# print(\"Done loading observations (%fs).\" % (time() - load_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # TODO Remove me on new pickled obs file\n",
    "# obs_data.sort_values('valid_datetime', axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Query netatmo data from disk.\n",
    "netatmo_path = '/Volumes/various/netatmo/data/'\n",
    "print(\"Querying NetAtmo file system engine..\")\n",
    "response = file_io.query(netatmo_path, request)\n",
    "data_map = response.data_map\n",
    "print(\"%d stations loaded.\" % len(data_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "db_connector = mongodb.MongoDBConnector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations = list(db_connector.db.stations.find(\n",
    "    {},\n",
    "    {'_id': 0}\n",
    "))\n",
    "data_map = {s['station_id']: Station.fromdict(s) for s in stations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stations_with_rain = {}\n",
    "for station_id in data_map:\n",
    "    station = data_map[station_id]\n",
    "    if station.hydro_module is not None:\n",
    "        stations_with_rain[station_id] = station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Insert one station\n",
    "station_ids = list(data_map.keys())\n",
    "sample_station = data_map[station_ids[1]]\n",
    "pprint(sample_station.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# \n",
    "query = {'_id': 'baz'}\n",
    "# TODO Only do $push if a measurement is present.\n",
    "update = {\n",
    "    '$setOnInsert': {\n",
    "        '_id': 'baz',\n",
    "        'elevation': sample_station.elevation,\n",
    "        'latitude': sample_station.latitude,\n",
    "        'longitude': sample_station.longitude,\n",
    "        'station_id': sample_station.station_id,\n",
    "        'thermo_module': sample_station.thermo_module,\n",
    "        'hydro_module': sample_station.hydro_module\n",
    "    },\n",
    "    '$push': {\n",
    "            'thermo_module.humidity': {'$each': sample_station.thermo_module['humidity']},\n",
    "            'thermo_module.pressure': {'$each': sample_station.thermo_module['pressure']},\n",
    "            'thermo_module.temperature': {'$each': sample_station.thermo_module['temperature']},\n",
    "            'thermo_module.valid_datetime': {'$each': sample_station.thermo_module['valid_datetime']}\n",
    "    }}\n",
    "result = db_connector.db.stations.update_one(query, update, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Query one station\n",
    "from pprint import pprint\n",
    "# returned_sample_station = db_connector.db.stations.find_one({'_id': station_ids[1]})\n",
    "returned_sample_station = db_connector.db.stations.find_one({'_id': 'baz'})\n",
    "pprint(returned_sample_station)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find all stations\n",
    "pprint(list(db_connector.db.stations.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Insert all stations\n",
    "for station_id in data_map:\n",
    "    db_connector.insert_stations(station_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cursor = db_connector.db.stations.find({'station_id': {'$in': station_ids}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "station_id = station_ids[1]\n",
    "# pprint(data_map[station_id].__dict__)\n",
    "results = db_connector.db.stations.update_one({'station_id': '1337'}, {'$push':{'thermo_module.humidity': 79, 'thermo_module.pressure': 1017.3, 'temperature': 7.8, 'valid_datetime': datetime(2016, 3, 27, 3, 56, 6)}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dump NetAtmo data to a single file for caching.\n",
    "# with open('data/netherlands_3-25_4-15.pkl', 'wb') as f:\n",
    "with open('data/precip_experiment3.pkl', 'wb') as f:\n",
    "    pickle.dump(data_map, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load cached NetAtmo data.\n",
    "load_start = time()\n",
    "# with open(\"data/netherlands_3-25_4-15.pkl\", \"rb\") as f:\n",
    "with open(\"data/precip_experiment3.pkl\", \"rb\") as f:\n",
    "    data_map = pickle.load(f)\n",
    "print(\"Done loading NetAtmo data (%fs).\" % (time() - load_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bson import json_util\n",
    "import json\n",
    "\n",
    "with open(\"data/netatmo_station_data.json\", \"w\") as f:\n",
    "    for station_id in data_map:\n",
    "        station = data_map[station_id]    \n",
    "        json.dump(\n",
    "            station.__dict__, f, \n",
    "            default=json_util.default,\n",
    "            sort_keys=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hydro_stats = []\n",
    "for station_id in data_map:\n",
    "    if data_map[station_id].hydro_module is not None:\n",
    "        hydro_stats.append(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thermo_stats = []\n",
    "for station_id in data_map:\n",
    "    if data_map[station_id].thermo_module is not None:\n",
    "        thermo_stats.append(station_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data_map[hydro_stats[50]].hydro_module)\n",
    "df_hourly = df[['hourly_rain_sum', 'time_hour_rain']]\n",
    "df_hourly.set_index('time_hour_rain').plot()\n",
    "df_hourly.set_index('time_hour_rain').cumsum().plot()\n",
    "df_daily = df[['daily_rain_sum', 'time_day_rain']]\n",
    "df_daily.plot()\n",
    "df_day_to_hour = df_daily.diff()\n",
    "df_day_to_hour[df_day_to_hour.daily_rain_sum < 0] = 0\n",
    "df_day_to_hour.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Select stations at or near a specified station id\n",
    "query_station_id = 6260  # WMO station to search nearby\n",
    "radius = 5000  # Search radius in meters\n",
    "\n",
    "# Do search and interpolation\n",
    "query_station_location = station_meta_data.loc[station_meta_data.stationId == query_station_id, [\"latitude\", \"longitude\"]]\n",
    "print(\"Query station location: lat %f, lon %f\" % (query_station_location.latitude, query_station_location.longitude))\n",
    "\n",
    "near_station_map = select_near(\n",
    "                                data_map, \n",
    "                                query_station_location.latitude, query_station_location.longitude,\n",
    "                                radius=radius\n",
    "                              )\n",
    "\n",
    "print(\"%d stations near WMO station %d.\" % (len(near_station_map), query_station_id))\n",
    "netatmo.resample_and_interpolate(near_station_map, request.time_resolution)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Construct a map plot of the query station and its found nearby NetAtmo stations.\n",
    "def plot_local_map(query_station_location, near_station_map): \n",
    "    query_lat = list(query_station_location.latitude.values)\n",
    "    query_lon = list(query_station_location.longitude.values)\n",
    "    map_1 = folium.Map(\n",
    "        query_lat + query_lon,\n",
    "        zoom_start=13, \n",
    "        tiles='stamentoner'\n",
    "    )\n",
    "    # Draw point of interest\n",
    "    folium.Marker(query_lat + query_lon, icon=folium.Icon(color='red')).add_to(map_1)\n",
    "\n",
    "    # Draw search radius\n",
    "    folium.CircleMarker(\n",
    "        query_lat + query_lon,\n",
    "        radius=radius,\n",
    "        color='#3186cc',\n",
    "        fill_color='#C7DDF0',\n",
    "       ).add_to(map_1)\n",
    "\n",
    "    for station_id in near_station_map:\n",
    "        station = near_station_map[station_id]\n",
    "        folium.Marker([station.latitude, station.longitude], popup=\"Station id: N%d\" % station.alias).add_to(map_1)\n",
    "    map_1\n",
    "    \n",
    "plot_local_map(query_station_location, near_station_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting of timeseries of nearby NetAtmo stations.\n",
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(200, 10), sharex=True)\n",
    "ax1.set_xlabel(\"Valid datetime\")\n",
    "\n",
    "element = 'temperature'\n",
    "show_cloudiness = False\n",
    "show_radiation = False\n",
    "show_grass = False\n",
    "show_merged = False\n",
    "\n",
    "show_individual = True\n",
    "\n",
    "# Construct observation series\n",
    "gp = obs_data.groupby('station_id')\n",
    "wmo_obs = gp.get_group(query_station_id)\n",
    "if element == 'pressure':\n",
    "    knmi_name = 'p0'\n",
    "elif element == 'temperature':\n",
    "    knmi_name = 'ta'\n",
    "elif element == 'humidity':\n",
    "    knmi_name = 'rh'\n",
    "elif element == 'cloudiness':\n",
    "    knmi_name = 'n'\n",
    "wmo_obs_el = wmo_obs[wmo_obs.element_name==knmi_name]\n",
    "\n",
    "print(\"Station %d has %d observations for element %s\" % (query_station_id, len(wmo_obs_el), element))\n",
    "\n",
    "# == Data processing ==\n",
    "merged_data = None  # Container for mean data\n",
    "for station_id in near_station_map:\n",
    "    station = near_station_map[station_id].thermo_module\n",
    "    station_alias = 'Station %d' % near_station_map[station_id].alias\n",
    "    if element in station:\n",
    "        if merged_data is None:\n",
    "            merged_data = station[[element]].add_suffix(\"_N%d\" % near_station_map[station_id].alias)\n",
    "        else:\n",
    "            merged_data = merged_data.join(station[[element]].add_suffix(\"_N%d\" % near_station_map[station_id].alias),\n",
    "                                           how='outer')\n",
    "\n",
    "# # Compute features for quality check\n",
    "# val_range = (merged_data.max(axis=0) - merged_data.min(axis=0))\n",
    "# val_sd = merged_data.std(axis=0)\n",
    "# val_mean = merged_data.mean(axis=0)\n",
    "\n",
    "# # Merge features\n",
    "# station_features = pd.concat([val_range, val_sd, val_mean], axis=1)\n",
    "# station_features.columns = ['range', 'sd', 'mean']\n",
    "\n",
    "# # The 'is_inside' check currently only works for temperature.\n",
    "# if element =='temperature':\n",
    "#     station_features['is_inside'] = np.logical_and(station_features.sd < 2, station_features.range < 10)\n",
    "# else:\n",
    "#     station_features['is_inside'] = False\n",
    "\n",
    "# # Select good stations\n",
    "# print(\"%d stations are installed inside and are discarded.\" % station_features.is_inside.sum())\n",
    "# outside_station_names = list(station_features[~station_features.is_inside].index)\n",
    "# merged_data = merged_data[outside_station_names]\n",
    "\n",
    "# # == Two-tailed Z-test on every date ==\n",
    "# alpha = 0.04\n",
    "# # Calculate mean per row\n",
    "# mean_per_date = merged_data.mean(axis=1)\n",
    "# std_per_date = merged_data.std(axis=1)\n",
    "# # Calculate CDF per row\n",
    "# merged_cdf_vals = merged_data.apply(\n",
    "#     norm.cdf,\n",
    "#     axis=0,\n",
    "#     loc=mean_per_date,\n",
    "#     scale=std_per_date\n",
    "# )\n",
    "# # Filter out values above threshold\n",
    "# merged_data = merged_data.mask(\n",
    "#     merged_cdf_vals.applymap(lambda x: (x > 1 - (0.5 * alpha)) or (x < (0.5 * alpha)))\n",
    "# )\n",
    "\n",
    "# == Compute root mean squared error ==\n",
    "# obs_copy = wmo_obs_el.copy()\n",
    "# obs_copy.set_index('valid_datetime', inplace=True)\n",
    "# wmo_netatmo = merged_data.mean(axis=1).to_frame(name='netatmo_' + element).join(obs_copy, how='outer')\n",
    "# wmo_netatmo.rename(\n",
    "#     columns={\n",
    "#         'value': 'wmo_' + element\n",
    "#     },\n",
    "#     inplace=True\n",
    "# )\n",
    "# wmo_netatmo.drop(['element_name', 'station_id'], axis=1, inplace=True)\n",
    "# error = wmo_netatmo['netatmo_' + element] - wmo_netatmo['wmo_' + element]\n",
    "# mae = error.abs().mean()\n",
    "# rmse = np.sqrt(error.apply(np.square).mean())\n",
    "# print(\"MAE Merged NetAtmo vs. WMO: %f\" % mae)\n",
    "# print(\"RMSE Merged NetAtmo vs. WMO: %f\" % rmse)\n",
    "\n",
    "# == First subplot == \n",
    "if show_individual:\n",
    "    for name in merged_data:    \n",
    "        p = ax1.plot(merged_data.index.values, merged_data[name].values, marker='x', label=name)\n",
    "            \n",
    "# Plot mean temperature of stations\n",
    "# if show_merged:\n",
    "#     ax1.plot(merged_data.index.values, merged_data.mean(axis=1), label='NetAtmo Mean', linewidth=2, color='cyan', marker='o')\n",
    "#     ax1.fill_between(\n",
    "#         merged_data.index.values, \n",
    "#         merged_data.mean(axis=1) - merged_data.std(axis=1),\n",
    "#         merged_data.mean(axis=1) + merged_data.std(axis=1),\n",
    "#         color='cyan'\n",
    "#     )\n",
    "    \n",
    "# nr_merged_stations = 0 if merged_data is None else len(merged_data.columns)\n",
    "# print(\"%d stations were merged.\" % nr_merged_stations)\n",
    "        \n",
    "# Plot element observations\n",
    "ax1.plot(wmo_obs_el['valid_datetime'].values, wmo_obs_el['value'].values, color='red', marker='o', markevery=6, label='WMO T')\n",
    "\n",
    "# Plot radiation observations\n",
    "if show_radiation:\n",
    "    wmo_obs_qg = wmo_obs[wmo_obs.element_name=='qg']\n",
    "    ax12 = ax1.twinx()\n",
    "    ax12.plot(wmo_obs_qg['valid_datetime'].values, wmo_obs_qg['value'].values, color='green', marker='o', markevery=6, label='WMO QG')\n",
    "    ax12.set_ylabel(\"Incoming radiation\")\n",
    "    \n",
    "if show_grass:\n",
    "    wmo_obs_qg = wmo_obs[wmo_obs.element_name=='tgn']\n",
    "    ax1.plot(wmo_obs_qg['valid_datetime'].values, wmo_obs_qg['value'].values, color='blue', marker='o', markevery=6, label='WMO TGN')\n",
    "\n",
    "if show_cloudiness:\n",
    "    wmo_obs_n = wmo_obs[wmo_obs.element_name=='n']\n",
    "    ax1.plot(wmo_obs_n['valid_datetime'].values, wmo_obs_n['value'].values, color='blue', marker='o', markevery=6, label='WMO N')\n",
    "\n",
    "    \n",
    "# == Second subplot ==\n",
    "# # Take difference over rows\n",
    "# merged_data_diff = merged_data.diff(axis=0)\n",
    "# # Plot station diffs\n",
    "# if show_individual:\n",
    "#     for name in merged_data:\n",
    "#         ax2.plot(merged_data_diff.index.values, merged_data_diff[name].values, marker='x', label=name)\n",
    "# # Plot WMO diff\n",
    "# ax2.plot(wmo_obs_el['valid_datetime'].values, wmo_obs_el['value'].diff().values, color='red', marker='o', markevery=6, label='WMO T')\n",
    "\n",
    "# Plot cleanup\n",
    "fig.autofmt_xdate()\n",
    "plt.xticks(wmo_obs_el['valid_datetime'].values[0::144])  # Make ticks daily\n",
    "plt.xlim([min(wmo_obs_el['valid_datetime'].values), max(wmo_obs_el['valid_datetime'].values)])\n",
    "ax1.set_title(\"%s timeseries\" % element)\n",
    "ax2.set_title(\"%s diff timeseries\" % element)\n",
    "ax2.set_ylabel(\"diff per hour\")\n",
    "ax1.grid('on')\n",
    "ax2.grid('on')\n",
    "# ax1.legend(numpoints=1)\n",
    "# ax2.legend(numpoints=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
